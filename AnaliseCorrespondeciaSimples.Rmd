---
title: "Análise de Correspondências Simples"
output:
  html_document:
    df_print: paged
---

Análise de Correpondência é uma técnica não supervisionada que busca diagnosticar relação entre variáveis qualitativas, usando para tal tabelas de contigência com as frequencias das associações.
Na simples temos apenas duas variáveis e chamamos de ANACOR.QUando há mais que 2 chamamos de análise de corrrespondencia multipla.

Livro Clássico: Correspondence Analysis Handbook. Autor: Benzecri 

Dados do exemplo:
```{R}
data = read.csv("data/perfil_investidor.csv")
```

As duas variáveis serão perfil e aplicação.

Tabela de frequência observadas:
```{R}
tab = table(data$perfil,data$aplicacao)
print(tab)
```


Teste qui-quadradro: associação entre as variáveis se dão de forma aleatória?

Hipotose nula: as variáveis qualitativas se associam de forma aleatória
hipotese alternativa: as variáveis qualitativas Não se associam de forma aleatória

```{r message=FALSE, warning=FALSE}
teste_qui_quadrado <- chisq.test(tab)
teste_qui_quadrado
```

Adicionado os totais das linahs e colunas:
```{R}
tabela = tab
tabela = cbind(tabela, Total = rowSums(tabela))
tabela = rbind(tabela, Total = colSums(tabela))
print(tabela)
```

Tabela de frequências esperadas (probabilidades):

```{R}
tabela_freq_esperadas = data.frame(matrix(NA, nrow=3, ncol=3))
for (row in 1:3){
  for(col in 1:3){
    tabela_freq_esperadas[row,col] = ( tabela[row,4]*tabela[4,col] )/ tabela[4,4]
  }
}
colnames(tabela_freq_esperadas) = colnames(tab)
rownames(tabela_freq_esperadas) = rownames(tab)
tabela_freq_esperadas
```
O teste qui-quadrado também nos fornece as frequencias esperadas:

```{r}
teste_qui_quadrado$expected
```

Matriz de resíduos (diferença do observado com o esperado, assim a soma dos resíduos sempre será zero):

```{r}
residuos =  tab-tabela_freq_esperadas
print(residuos)
```

Qui-quadradro será dado pela soma dos residuos ao quadrado divido pelos valores esperados:

```{r}
tabela_qui_quadrado = residuos^2 / tabela_freq_esperadas
sum(tabela_qui_quadrado)
```

31.76416 está acima ou abaixo do valor crítico?
graus de liberdade (df) da tabela de contigência = (linhas-1)*(colunas-1) = 4

5% de significância (= 95% de confiânca):
```{r}
qchisq(p=0.05, df =4, lower.tail=F)
```

9.487729 é chamado qui-quadrado crítico.
Como 31.76416 é maior que 9.487729 rejeitamos a hipótese nula e ficamos com a hipótese alternativa. Podemos fazer o contrário também, encontrar o p-value para 31.76416:

```{r}
pchisq(q=31.76416, df =4, lower.tail=F)
```
2.137599e-06 é bem menor que 0.05, assim rejeitamos a hipótese nula.

resíduos padrozinados:
```{r}
residuos_padronizados = residuos/sqrt(tabela_freq_esperadas)
residuos_padronizados
```

A função qui-quadrado também nos fornece os resíduos padronizados:
```{r}
teste_qui_quadrado$residuals
```

Resíduos padronizados ajustados:

```{r}
residuos_padronizados_ajustados = data.frame(matrix(NA, nrow=3, ncol=3))
for (row in 1:3){
  for(col in 1:3){
    denominador = ( (1-tabela[row,4]/tabela[4,4]) * (1-tabela[4,col]/tabela[4,4])  )
    residuos_padronizados_ajustados[row,col] = residuos_padronizados[row,col] / sqrt(denominador)
  }
}
colnames(residuos_padronizados_ajustados) = colnames(tab)
rownames(residuos_padronizados_ajustados) = rownames(tab)
residuos_padronizados_ajustados

```

O teste qui-quadrado também nos fornece os resíduos padronizados ajustados:

```{r}
teste_qui_quadrado$stdres
```

Benzecri encontrou que os resíduos padronizados ajustados em simulação com grande volume de dado regridem numa normal padrão.

Usaremos os resíduos padronizados ajustado mairoes que 1,96 (indica associação intensa entre as duas variáveis) para montar o mapa perceptual.


Viés do teste qui-quadrado: quanto maior amostra maior o teste qui-quadrado. A Inércia principal total tenta resolver isso dividindo o qui-quadrado pelo número de observações:

```{r}
inercia_principal_total = teste_qui_quadrado$statistic/nrow(data)
inercia_principal_total
```

Agora vamos decompor a inércia principal total.
Os autovalores, que mostram a quantidade de dimensões que vamos capturar, serão calculados a partir de uma matriz *W* que é dada por $W=A'A$. Sendo *A* dado por: .

$A=D_l^2(P-lc')D_c^2$

Essa matriz A irá capturar o valor da inertia principal total, isto é, 0.3176416 

P é a matriz de frequências relativas:

```{r}
P = 1/nrow(data) * tab
P
```

Profile  é uma medida de quanto cada categoria está puxando as demais.

Column profiles (massas da linhas):

```{r}
column_profiles = data.frame(matrix(NA, nrow=3, ncol=3))
for (row in 1:3){
  for(col in 1:4){
    column_profiles[row,col] =  tabela[row,col]/tabela[4,col]
  }
}
colnames(column_profiles) = c(colnames(tab),'Dl')
rownames(column_profiles) = rownames(tab)
column_profiles
```

Row profiles (massas da colunas)

```{r}
row_profiles = data.frame(matrix(NA, nrow=3, ncol=3))
for (row in 1:4){
  for(col in 1:3){
    row_profiles[row,col] =  tabela[row,col]/tabela[row,4]
  }
}
colnames(row_profiles) = colnames(tab)
rownames(row_profiles) = c(rownames(tab),'Dc')
row_profiles
```

Matriz $D_l$ é a diagonalização dos colunm profiles:
```{r}
Dl = diag(column_profiles$Dl)
Dl
```

Matriz $D_c$é a diagonalização dos row profiles:
```{r}
Dc = diag(row_profiles[4,])
Dc
```

Matriz lc transposta
```{r}
dl_matrix = as.matrix(column_profiles$Dl)
rownames(dl_matrix) = rownames(tab)
lc = dl_matrix %*% as.matrix(row_profiles[4,])
lc
```

Matriz A
```{r}
A = diag(diag(Dl) ^ (-1/2)) %*% (P - lc) %*% diag(diag(Dc) ^ (-1/2))
A
```

Mas também poderíamos encontrar A assim (que é muito mais fácil):
```{r}
A = residuos_padronizados / sqrt(nrow(data))
A
```
Matriz W:
```{r}
W <- t(A) %*% as.matrix(A)
W
```

Extraindo os autovalores da matriz W:
```{R}
autovalores <- eigen(W)
autovalores$values
```

A quantidade de autovalores que vamos extrair será o menor de linha-1 ou coluna-1. No nosso caso 2. perceba que o terceiro autovalor tende a zero (8.326673e-17). Somando os dois primeiro autovalores  2.332149e-01+8.442678e-02 = 0.3176417 chegamos no valor da inertia principal total, isto é, 0.3176416 

A soma do autovalores nos da o valor da inércia principal total explicada.

O primeiro auto valor explica 73,4% da inércia principal total:

```{R}
autovalores$values[1]/sum(autovalores$values)
```
inercia principal total explicada:
```{R}
inercia_principal_total_explicada <- autovalores$values[1:2] / inercia_principal_total
inercia_principal_total_explicada
```

Para encontrarmos as coordenadas do mapa perceptual temos que fazer a decomposição do valor singular da Matriz A:
```{R}
dimensao = min(nrow(tab),ncol(tab))
decomposicao <- svd(x = A, nu = dimensao,nv = dimensao)
decomposicao
```

Raiz quadrada dos autovalores:
```{r}
decomposicao$d
```

Raiz quadrada dos autovalores:
```{r}
decomposicao$d
sqrt(autovalores$values)
```
Vetor u:
```{r}
Am = as.matrix(A)
eigen(Am %*% t(Am))
decomposicao$u
```

Vetor v:
```{r}
eigen(t(Am) %*% Am)
decomposicao$v
```

X e Y para perfil:
```{r}
Xl_perfil <- diag((decomposicao$d[1]) * diag(diag(Dl)^(-1/2)) * decomposicao$u[,1])
Xl_perfil

#Variável em linha - coordenada no eixo das ordenadas
Yl_perfil <- diag((decomposicao$d[2]) * diag(diag(Dl)^(-1/2)) * decomposicao$u[,2])
Yl_perfil
```
X e Y para aplicação:
```{r}
Xc_aplicacao <- diag((decomposicao$d[1]) * diag(diag(Dc)^(-1/2)) * decomposicao$v[,1])
Xc_aplicacao

Yc_aplicacao <- diag((decomposicao$d[2]) * diag(diag(Dc)^(-1/2)) * decomposicao$v[,2])
Yc_aplicacao
```

Guardando as coordenadas de cada categoria e de cada variável em um dataframe:

```{r}
data$perfil = as.factor(data$perfil)
data$aplicacao = as.factor(data$aplicacao)
coordenadas <- data.frame(Categorias = cbind(c(levels(data$perfil), levels(data$aplicacao))),
                          Dim1 = cbind(c(Xl_perfil, Xc_aplicacao)),
                          Dim2 = cbind(c(Yl_perfil, Yc_aplicacao)))
coordenadas
```

Adicionando a coluna com o tipo da categoria:
```{r message=FALSE, warning=FALSE}
library(tibble)
library(tidyverse)

variaveis <- apply(data[,2:3],
                   MARGIN =  2,
                   FUN = function(x) nlevels(as.factor(x)))

matrix_mapa_perceptual <- data.frame(coordenadas,Variaveis = rep(names(variaveis), variaveis))

matrix_mapa_perceptual = rename(matrix_mapa_perceptual,Category = 1)

matrix_mapa_perceptual = rownames_to_column(matrix_mapa_perceptual)
matrix_mapa_perceptual
```

Mapa perceptual bidimensional:
```{r}
library(ggplot2)
library(ggrepel)

lab_x = paste("Dimension 1:", paste0(round(inercia_principal_total_explicada[1] * 100, digits = 2), "%"))
lab_y = paste("Dimension 2:", paste0(round(inercia_principal_total_explicada[2] * 100, digits = 2), "%"))

ggplot(data = matrix_mapa_perceptual, 
       aes(x = Dim1, y = Dim2, label = Category, color = Variaveis)) +
  geom_point() +
  geom_label_repel() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = lab_x, y = lab_y) 

```


