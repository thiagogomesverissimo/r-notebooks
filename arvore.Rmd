---
title: "árvore"
output:
  html_document:
    df_print: paged
---

exemplo de árvore de decisão com resposta binária:

```{r}
data = read.csv("data/titanic.csv")

data$Survived = as.factor(data$Survived)
data$Survived = relevel(data$Survived, ref="N")

data$Pclass = as.factor(data$Pclass)
data$Sex = as.factor(data$Sex)
data$Embarked = as.factor(data$Embarked)
```

Cálculo da impureza com Gini para o nó raiz:

```{r}
tab = table(data$Survived) # Y=549 N=342 

pY = tab[1]/nrow(data)
pN = tab[2]/nrow(data)

gini_raiz = 1 - (pY^2 + pN^2)
gini_raiz
```

Cálculo da impureza com Gini para o nó da variável sexo:

```{r}
male = data[data$Sex == 'male',]
female = data[data$Sex == 'female',]

# male
tab = table(male$Survived) # Y=549 N=342 
pY = tab[1]/nrow(male)
pN = tab[2]/nrow(male)
gini_male = 1 - (pY^2 + pN^2)

# female
tab = table(female$Survived) # Y=549 N=342 
pY = tab[1]/nrow(female)
pN = tab[2]/nrow(female)
gini_female = 1 - (pY^2 + pN^2)

# média ponderada do Gini

gini_sexo = (gini_male*nrow(male) + gini_female*nrow(female))/nrow(data)
gini_sexo
```

Teríamos que calcular o Gini para cada variável e então escolher a quebra na variável de menor Gini (impureza).

O Gini varia de 0 até 0,5, sendo zero a impureza minima (overfitting, acertamos tudo) e 0.5 erramos tudo.

```{r}
p = data.frame(p1 = seq(0,1, length=100))
p$p2 = 1- p$p1
p$gini = 1 - (p$p1^2 + p$p2^2)
plot(p$p1, p$gini)
```

A entropia é um parâmetro parecido como o Gini 


Descritiva:

```{r}
library(Rmisc)
data$survived <- as.integer(data$Survived=="Y")
data_summary_sex = summarySE(data, measurevar="survived", groupvars=c("Sex"))
data_summary_sex
```

Por sexo:

```{r}
library(tidyverse)
 ggplot(data_summary_sex) + 
   geom_bar(aes(x=Sex, fill=as.factor(Sex), weight = N/nrow(data),)) +
    geom_errorbar(aes(x=Sex, y=survived, ymin=survived-se, ymax=survived+se, colour='1'), width=.1) +
   
    geom_point(aes(x=Sex, y=survived, colour='1', group='1')) +
    geom_line(aes(x=Sex, y=survived, colour='1', group='1')) +
    
    scale_color_viridis_d(direction = -1, begin=0, end=.25) +
    scale_fill_viridis_d(direction = -1, begin=.85, end=.95) +
    
    theme(panel.background = element_rect(fill = "white", colour = "grey", linetype = "solid"),
          panel.grid.major = element_line(size = 0.15, linetype = 'solid', colour = "grey")) + 
   
    theme(legend.position = "none") +
    xlab("Sexo") + 
    ylab("Taxa de sobreviventes") +
    scale_y_continuous(sec.axis = sec_axis(~ . * nrow(data) , name = "Frequência"), labels = scales::percent)
```


Por idade:
```{r}
library(Rmisc)
library(tidyverse)
library(gtools)

data$quant_idade <- quantcut(data$Age, 5)
data_summary_idade = summarySE(data, measurevar="survived", groupvars=c("quant_idade"))
data_summary_idade

ggplot(data_summary_idade) + 
  geom_bar(aes(x=quant_idade, fill=as.factor(quant_idade), weight = N/nrow(data),)) +
  geom_errorbar(aes(x=quant_idade, y=survived, ymin=survived-se, ymax=survived+se, colour='1'), width=.1) +
 
  geom_point(aes(x=quant_idade, y=survived, colour='1', group='1')) +
  geom_line(aes(x=quant_idade, y=survived, colour='1', group='1')) +
  
  scale_color_viridis_d(direction = -1, begin=0, end=.25) +
  scale_fill_viridis_d(direction = -1, begin=.85, end=.95) +
  
  theme(panel.background = element_rect(fill = "white", colour = "grey", linetype = "solid"),
        panel.grid.major = element_line(size = 0.15, linetype = 'solid', colour = "grey")) + 
 
  theme(legend.position = "none") +
  xlab("Idades") + 
  ylab("Taxa de sobreviventes") +
  scale_y_continuous(sec.axis = sec_axis(~ . * nrow(data) , name = "Frequência"), labels = scales::percent)
```

árvore:

```{r}
library(rpart)
arvore <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                data=data,
                parms = list(split = 'gini'),
                method='class'
)
paleta = scales::viridis_pal(begin=.75, end=1)(20)
rpart.plot::rpart.plot(arvore, box.palette = paleta) 
```

- probabilidade de sobrevivência global: 38%
- probabilidade de sobrevivência de homens: 19%
- probabilidade de sobrevivência de homens com menos de 6,5 anos: 67%

## dividindo a base em treino e teste

```{r}
set.seed(123)
treino_bool <- stats::runif(dim(data)[1])>.25

treino <- data[treino_bool,]
teste  <- data[!treino_bool,]
```

árvove "livre" permite uma obsevação nos nós ou folhas e tem custo cp zero (permite overfitting):

```{r}
arvore <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                data=treino,
                parms = list(split = 'gini'),
                method='class',
                xval=5,
                control = rpart.control(cp = 0, minsplit = 1, maxdepth = 30)
)
arvore$variable.importance
```

predição:

```{r message=FALSE, warning=FALSE}
library(caret)
previsto = stats::predict(arvore, teste)

previsto = as.data.frame(previsto)
teste = cbind(teste,previsto)

previsto$SurvivedPredicted = "N"
previsto[previsto[,2] > 0.5,]$SurvivedPredicted = "Y"

teste = cbind(teste,SurvivedPredicted = previsto$SurvivedPredicted)
teste$SurvivedPredicted = as.factor(teste$SurvivedPredicted)
teste$SurvivedPredicted = relevel(teste$SurvivedPredicted , ref="N")

confusionMatrix(teste$SurvivedPredicted, teste$Survived, positive="N")
```

ROC:

```{r}
library(caret)
rocdata = data.frame(obs=teste$Survived, 
                     pred=teste$SurvivedPredicted, 
                     Y = teste$Y, 
                     N = teste$N)

twoClassSummary(rocdata, lev=levels(rocdata$obs))
```

Curva ROC:

```{r}
library(plotROC)
ggplot(rocdata, aes(d = obs, m = Y, colour='a')) + 
  plotROC::geom_roc(n.cuts = 0) +
  scale_color_viridis_d(direction = -1, begin=0, end=.25) +
  theme(legend.position = "none") +
  ggtitle("Curva ROC")
```

## pós-poda (Grid Search)

Dividimos a base de treino em k pedaços (k-fold) e recursivamente fixando um valor de cp testamos um pedaço como sendo validação e os demais k-1 como treino. Todos pedaços k são considerados como validação em algum momento e depois fazemos a média de uma métrica de desempenho, como por exemplo a acurácia. Testamos esse procedimento para diversos CP e procuramos o menor: 

```{r}
poda = rpart::printcp(arvore)
poda
```

```{r}
plotcp(arvore)
```

```{r}
cp_min <- poda[which.min(poda[,'xerror']),'CP']
cp_min
```

Dado que temos o menor CP recosntruímos a árvore:

```{r}
set.seed(123)
arvore_poda <- rpart::rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                            data=treino,
                            method='class',
                            xval=0,
                            control = rpart.control(cp = cp_min, 
                                                    minsplit = 1, 
                                                    maxdepth = 30)
)
```

predição:

```{r message=FALSE, warning=FALSE}
library(caret)
previsto = stats::predict(arvore_poda, teste)

previsto = as.data.frame(previsto)
teste = cbind(teste,previsto)

previsto$SurvivedPredicted = "N"
previsto[previsto[,2] > 0.5,]$SurvivedPredicted = "Y"

teste = cbind(teste,SurvivedPredicted = previsto$SurvivedPredicted)
teste$SurvivedPredicted = as.factor(teste$SurvivedPredicted)
teste$SurvivedPredicted = relevel(teste$SurvivedPredicted , ref="N")

confusionMatrix(teste$SurvivedPredicted, teste$Survived, positive="N")
```
área ROC:

ROC:

```{r}
library(caret)
rocdata = data.frame(obs=teste$Survived, 
                     pred=teste$SurvivedPredicted, 
                     Y = teste$Y, 
                     N = teste$N)

twoClassSummary(rocdata, lev=levels(rocdata$obs))
```

Curva ROC:

```{r}
library(plotROC)
ggplot(rocdata, aes(d = obs, m = Y, colour='a')) + 
  plotROC::geom_roc(n.cuts = 0) +
  scale_color_viridis_d(direction = -1, begin=0, end=.25) +
  theme(legend.position = "none") +
  ggtitle("Curva ROC")
```


Cross-validation ao longo dos CPs 

```{r}
library(caret)
stats <- data.frame(NULL)
# Loop ao longo dos valores de CP
cptab= poda
for (cp in cptab[2:dim(cptab)[1],'CP']){
  
  # Treinar a árvore alterando o cp
  arvore <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                  data=treino,
                  method='class',
                  control = rpart.control(cp = cp, 
                                          minsplit = 1, 
                                          maxdepth = 30),
                  xval=0
  )
  
  # Avaliar a árvore na base de treino
  p_treino = predict(arvore, treino)
  c_treino = factor(ifelse(p_treino[,2]>.5, "Y", "N"))
  
  aval_treino <- data.frame(obs=treino$Survived, 
                            pred=c_treino,
                            Y = p_treino[,2],
                            N = 1-p_treino[,2]
  )
  aval_treino
  av_treino <- twoClassSummary(aval_treino, lev=levels(aval_treino$obs))
  
  # Avaliar base de teste  
  p_teste = predict(arvore, teste)
  c_teste = factor(ifelse(p_teste[,2]>.5, "Y", "N"))
  aval_teste <- data.frame(obs=teste$Survived, 
                           pred=c_teste,
                           Y = p_teste[,2],
                           N = 1-p_teste[,2]
  )
  
  av_teste <- twoClassSummary(aval_teste, lev=levels(aval_teste$obs))
  
  # Acumular as informações de cp e AUC para cada árvore  
  stat <- cbind(cp, ROC_treino=av_treino[1], ROC_teste=av_teste[1])
  stats <- rbind(stats, stat)
  
}
stats
```


```{r}
ggplot(stats) +
  geom_point(aes(x=cp, y=ROC_treino, col='treino')) +
  geom_point(aes(x=cp, y=ROC_teste, col='teste')) +
  scale_color_viridis_d(begin=.4, end=.8) +
  theme(legend.position = "bottom") +
  ggtitle("Curva ROC - base de treino") +
  ylab("AUC") +
  labs(colour='Base') 

```

## árvore com resposta contínua

Inventando dados aleatórios no formato de uma parábola:
```{r}
set.seed(123)
x <- seq(0,1, length.out=1000)

# y segue uma relação quadrática
a <- 0
b <- 10
c <- -10

y <- a + b*x + c*x**2 + rnorm(length(x), mean=0, sd=.1)

df <- data.frame(x, y)

p0 <- ggplot(df, aes(x,y)) + 
  geom_point(aes(colour='Observado')) +
  scale_color_viridis(discrete=TRUE, begin=0, end=.85, name = "Valor") +
  theme(legend.position="bottom",
        legend.spacing.x = unit(0, 'cm'))
p0
```
árvore de decisão:

```{r}
tree <- rpart(y~x, 
              data=df,
              control=rpart.control(maxdepth = 3, cp=0))

# Plotando a árvore
paleta = scales::viridis_pal(begin=.75, end=1)(20)
rpart.plot::rpart.plot(tree,
                       box.palette = paleta) # Paleta de cores
```

Predição:

```{r}
df['p'] = predict(tree, df)
df['r'] = df$y - df$p

# Valores esperados e observados
boost0_O_vs_E <- ggplot(df, aes(x,y)) + 
  geom_point(alpha=.7, size=.5, aes(colour='Observado')) +
  geom_path(aes(x,p, colour='Esperado')) + #Ploting
  scale_color_viridis(discrete=TRUE, begin=0, end=.8, name = "Dado: ") +
  theme_bw() +
  theme(legend.position="bottom") +
  # guides(colour = guide_legend(label.position = "bottom")) +
  labs(title="Valores observados vs esperados") +
  scale_y_continuous(name= "y") +
  scale_x_continuous(name= "x")

boost0_O_vs_E
```
Residuos:

```{r}
library(ggpubr)

boost0_res <- ggplot(df, aes(x,r)) + 
  geom_point(alpha=.7, size=.5, aes(colour='Resíduo')) +
  scale_color_viridis(discrete=TRUE, begin=0, end=.8, name = "Dado: ") +
  theme_bw() +
  theme(legend.position="bottom") +
  labs(title="Gráfico de resíduos") +
  scale_y_continuous(name= "r") +
  scale_x_continuous(name= "x")
boost0_res

ggpubr::ggarrange(boost0_O_vs_E, boost0_res, 
          # labels = c("A", "B"),
          ncol = 2, nrow = 1)
```

### árvore de decisão para y binária

```{r}
data = read.csv("data/semaforos.csv")
```

árvore de decisão

```{r}
CART_atrasos <- rpart(formula = atraso ~ .,
                        data = data,
                        control = rpart.control(minsplit = 1,
                                                cp = 0.01,
                                                maxdepth = 30),
                        parms = list(split = "information"),
                        method = "class")

rpart.plot(CART_atrasos, type = 1)

```

Data:

```{r}
data %>% 
  ggplot() +
  geom_point(aes(x = distância,
                 y = qtde_semáforos, 
                 color = atraso, 
                 shape = atraso), 
             size = 2) +
  labs(x = "Distância",
       y = "Quantidade de Semáforos",
       color = "Houve atraso?",
       shape = "Houve atraso?") +
  scale_color_viridis_d() +
  scale_x_continuous(breaks = c(9:30)) +
  scale_y_continuous(breaks = c(5:15)) +
  theme_bw()
```
## árovre de decisão para y contínua 

```{r}
data = read.csv("data/carros.csv")
```

Separando treino e validação:

```{r}
set.seed(123)
sample_index <- sample(x = 1:dim(data)[1],
                       size = nrow(data) * 0.7)

amostra_treino <- data[sample_index, ]
amostra_validacao <- data[-sample_index, ]

```

árvore:
```{r}
set.seed(123)
CART_carros <- rpart(formula = preco ~ .,data = amostra_treino)
rpart.plot(CART_carros, type = 1)
```

```{r}
printcp(CART_carros)
```

 Prunando a CART ---------------------------------------------------------

```{r}
prunned_carros <- prune(tree = CART_carros,
                        cp = CART_carros$cptable[which.min(
                          CART_carros$cptable[,"xerror"]),"CP"]
)
rpart.plot(prunned_carros, type = 1)
```


Salvando os fitted values da amostra treino 
```{r}
amostra_treino["fitted"] <- predict(object = prunned_carros,
                                    newdata = amostra_treino)

```


Aderência à variável Idade

```{r}
amostra_treino %>% 
  ggplot() +
  geom_point(aes(x = idade, 
                 y = preco, 
                 color = "Amostra Treino"), 
             alpha = 0.5) +
  geom_smooth(aes(x = idade, 
                  y = fitted, 
                  color = "Fitted Values"), 
              se = F,
              size = 1.5) +
  labs(color = "Legenda:",
       x = "Idade",
       y = "Preço") +
  scale_color_viridis_d(option = "cividis") +
  theme_bw()

```


Aderência à variável Ano de Montagem

```{r}
amostra_treino %>% 
  ggplot() +
  geom_point(aes(x = ano_montagem, 
                 y = preco, 
                 color = "Amostra Treino"), 
             alpha = 0.5) +
  geom_smooth(aes(x = ano_montagem, 
                  y = fitted, 
                  color = "Fitted Values"), 
              se = F,
              size = 1.5, method = "lm") +
  labs(color = "Legenda:",
       x = "Ano de Montagem",
       y = "Preço") +
  scale_color_viridis_d(option = "cividis") +
  theme_bw()

```

Aderência à variável Peso

```{r}
amostra_treino %>% 
  ggplot() +
  geom_point(aes(x = peso, 
                 y = preco, 
                 color = "Amostra Treino"), 
             alpha = 0.5) +
  geom_smooth(aes(x = peso, 
                  y = fitted, 
                  color = "Fitted Values"), 
              se = F,
              size = 1.5, method = "lm") +
  labs(color = "Legenda:",
       x = "Peso",
       y = "Preço") +
  scale_color_viridis_d(option = "cividis") +
  theme_bw()

```

Salvando os fitted values da amostra validação
```{r}
amostra_validacao["fitted"] <- predict(object = prunned_carros,
                                       newdata = amostra_validacao)

```

Aderência à variável Idade

```{r}
amostra_validacao %>% 
  ggplot() +
  geom_point(aes(x = idade, 
                 y = preco, 
                 color = "Amostra Validação"), 
             alpha = 0.5) +
  geom_smooth(aes(x = idade, 
                  y = fitted, 
                  color = "Fitted Values"), 
              se = F,
              size = 1.5) +
  labs(color = "Legenda:",
       x = "Idade",
       y = "Preço") +
  scale_color_viridis_d(option = "cividis") +
  theme_bw()

```


Aderência à variável Ano de Montagem

```{r}
amostra_validacao %>% 
  ggplot() +
  geom_point(aes(x = ano_montagem, 
                 y = preco, 
                 color = "Amostra Validação"), 
             alpha = 0.5) +
  geom_smooth(aes(x = ano_montagem, 
                  y = fitted, 
                  color = "Fitted Values"), 
              se = F,
              size = 1.5, method = "lm") +
  labs(color = "Legenda:",
       x = "Ano de Montagem",
       y = "Preço") +
  scale_color_viridis_d(option = "cividis") +
  theme_bw()


```


Aderência à variável Peso

```{r}
amostra_validacao %>% 
  ggplot() +
  geom_point(aes(x = peso, 
                 y = preco, 
                 color = "Amostra Validação"), 
             alpha = 0.5) +
  geom_smooth(aes(x = peso, 
                  y = fitted, 
                  color = "Fitted Values"), 
              se = F,
              size = 1.5, method = "lm") +
  labs(color = "Legenda:",
       x = "Peso",
       y = "Preço") +
  scale_color_viridis_d(option = "cividis") +
  theme_bw()
```


Avaliando a RMSE amostra treino:

```{r}
rmse(actual = amostra_treino$preco,
     predicted = amostra_treino$fitted) #amostra treino

```

Avaliando a RMSE amostra validação:

```{r}
rmse(actual = amostra_validacao$preco,
     predicted = amostra_validacao$fitted) #amostra validação

```

