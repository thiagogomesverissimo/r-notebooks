---
title: "Deep Learning"
output:
  html_document:
    df_print: paged
---

Pacotes:

```{r}
library("MASS")
library("rpart")
set.seed(0)
```

Vamos tentar prever o valor media de uma casa baseados das features disponíveis no dataset: 
Boston Housing Values in Suburbs of Boston

```{r}
data = Boston
data[is.na(data) == TRUE]
head(data)
```

Separando entre treinamento e teste:

```{r}
quebra = 0.8 * nrow(data)
train <- data.frame(data[1:quebra,])
test <- data.frame(data[(1+quebra): nrow(data),])
```

árvore de decisão (anova para regressão e class para classificação):
```{r}
arvore <- rpart(medv ~.,method="anova", data=train)
tree_predict <- predict(arvore,test)
mse_tree <- mean((tree_predict - test$medv)^2)
mse_tree
```

Normalizando segundo  x-min/(max-min), para diminuir a intensidade dos ruídos:

```{r}
set.seed(0)
maior_de_cada_coluna = apply(data, 2, max) 
menor_de_cada_coluna = apply(data, 2, min)
diff = maior_de_cada_coluna - menor_de_cada_coluna
scaled <- scale(data, center = menor_de_cada_coluna, scale = diff)
```

Separando treinamento e teste de forma aleatória:

```{r}
set.seed(0)
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
```

Rede neural:

```{r}
library(neuralnet)
nn <- neuralnet(medv~.,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
```

hidden: vector of integers specifying the number of hidden neurons (vertices) in each layer.
hidden=c(5,4,3,2): 

- Na primeira camada escondida 5 neurônios
- Na segunda camada escondida 4 neurônios
- Na terceira camada escondida 3 neurônios 
- Na quarta camada escondida 2 neurônios 

Previsão é com a função compute (e não com predict) e temos que desfazer o scale para compararmos com a variável original:

```{r}
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
```

Calculando erro quadrático médio:

```{r}
MSE_nn <- mean((pr.nn_ - test.r)^2)
MSE_nn
```
Plot do observado versus predito:

```{r}
plot(test_data$medv,type = 'l',col="red",xlab = "x", ylab = "Valor Residencia")
lines(pr.nn$net.result,col = "blue")
```
## exemplo de rede neural com classificação

Vamos prever a variável Private, que indica se a universidade é pública ou privada.

```{r}
library(ISLR)
library(neuralnet)
data <- College
private <- ifelse(data$Private == 'Yes', 1, 0)
```

Padronizar dados para melhor performance
```{r}
data <- data[,2:18]
max_data <- apply(data, 2, max) 
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
```

Inclui variável explicada (target)
```{r}
scaled$Private <- private
```

Separando treinamento e teste:

```{r}
set.seed(0)
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
```

Modelo rede neural com a opção linear.output=F pois agora estamos no contexto da classificação:

```{r}
set.seed(0)
nn <- neuralnet(Private ~ .,data=train_data,hidden=c(5,4),linear.output=F)
plot(nn)
```

Previsão:

```{r}
pr.nn <- compute(nn,test_data[,1:17])
pr.nn$net.result <- sapply(pr.nn$net.result,round,digits=0) # aqui seria melhor usar cutoff e curva ROC
pr.nn$net.result
```

Tabela de confusão:

```{r}
table(medido=test_data$Private,previsto=pr.nn$net.result)
```

Acurácia:

```{r}
(62+158) / (62+158+7+6)
```

árvore de classificação para compararmos

```{r}
set.seed(0)
fit_tree <- rpart(Private ~ .,method="class", data=train_data)
tree_predict <- predict(fit_tree,test_data,type = "class")
table(test_data$Private,tree_predict)
```

acurácia para árvore de classificação:

```{r}
(58+159) / (58+159+11+5)
```

camada densa: Conecta cada neurônio em uma camada com todos os neurônios da camada anterior.
Épocas: passagens completas do conjunto de dados devem ser usadas quando treinamos uma rede

Muitas camadas permitem identificar relações não lineares


#### aplicando rede reunal em um exemplo de resposta contínua:

Dado fake:

```{r}
library(viridis)
library(tidyverse)

x1 <- seq(0,1, length.out=1000)

# y segue uma relação quadrática com estes parâmetros
a <- 0
b <- 12.5
c <- -10

# Gerar y
set.seed(1729)
y1 <- a + b*x1 + c*x1**2 + rnorm(length(x1), mean=0, sd=.1)
df1 <- data.frame(x1, y1) # criar um dataframe
colnames(df1) <- c('x', 'y') #renomear colunas

# Gráfico dos dados gerados
p0 <- ggplot(df1, aes(x,y)) + 
  geom_point(aes(colour='Observado'), alpha=.5) +
  viridis::scale_color_viridis(discrete=TRUE, begin=0, end=.85, name = "Valor") +
  theme_bw() +
  theme(legend.position="bottom",
        legend.spacing.x = unit(0, 'cm'))
p0

```
Rede neural sem camada escondida:

```{r}
set.seed(123)
rn0 <- neuralnet::neuralnet(y ~ x, 
                 data=df1, 
                 threshold = 0.01,
                 act.fct = 'logistic'
                 )

plot(rn0)
```

Previsto versus observados:

```{r}
df1['pred0'] <- predict(rn0, df1)
boost0_O_vs_E <- ggplot(df1, aes(x,y)) + 
  geom_point(alpha=.7, size=.5, aes(colour='Observado')) +
  geom_path(aes(x,pred0, colour='Esperado')) + #Ploting
  scale_color_viridis(discrete=TRUE, begin=0, end=.8, name = "Dado: ") +
  theme_bw() +
  labs(title="Valores observados vs esperados") +
  scale_x_continuous(name= "x")

boost0_O_vs_E

```


Rede neural com camada escondidas (3 neuronios na camada 1 e 2 na e):

```{r}
set.seed(123)
rn1 <- neuralnet(y ~ x, 
                 data=df1, 
                 hidden = c(3,2))
plot(rn1)
```

Valores esperados e observados

```{r}
df1['pred1'] <- predict(rn1, df1)
boost0_O_vs_E <- ggplot(df1, aes(x,y)) + # gráfico base >> x vs y <<
  geom_point(alpha=.7, size=.5, aes(colour='Observado')) +
  geom_path(aes(x,pred1, colour='Esperado')) + # Gráfico sobreposto >> x vs pred
  scale_color_viridis(discrete=TRUE, begin=0, end=.8, name = "Dado: ") +
  theme_bw() +
  theme(legend.position="bottom") +
  labs(title="Valores observados vs esperados") +
  scale_y_continuous(name= "y") +
  scale_x_continuous(name= "x")

boost0_O_vs_E
```

